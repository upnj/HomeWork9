[
  {
    "objectID": "HomeWrok9.html",
    "href": "HomeWrok9.html",
    "title": "Home Work 9",
    "section": "",
    "text": "Let’s take the Part of the code from Home Work 8 as we used MLR model. Please note I am using the bike_data_clean dataset insead of daily_bike_data used in home work 8.\n#install.packages(\"baguette\")\n#install.packages(\"ranger\")\nlibrary(ranger)\n\nWarning: package 'ranger' was built under R version 4.4.2\n\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.4.2\n\n\nLoading required package: parsnip\n\n\nWarning: package 'parsnip' was built under R version 4.4.2\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.4.2\n\nlibrary(dplyr)\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.4.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ recipes      1.1.0     ✔ yardstick    1.3.1\n\n\nWarning: package 'dials' was built under R version 4.4.2\n\n\nWarning: package 'infer' was built under R version 4.4.2\n\n\nWarning: package 'modeldata' was built under R version 4.4.2\n\n\nWarning: package 'recipes' was built under R version 4.4.2\n\n\nWarning: package 'rsample' was built under R version 4.4.2\n\n\nWarning: package 'tune' was built under R version 4.4.2\n\n\nWarning: package 'workflows' was built under R version 4.4.2\n\n\nWarning: package 'workflowsets' was built under R version 4.4.2\n\n\nWarning: package 'yardstick' was built under R version 4.4.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(glmnet)\n\nWarning: package 'glmnet' was built under R version 4.4.2\n\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\nlibrary(rpart)\n\n\nAttaching package: 'rpart'\n\nThe following object is masked from 'package:dials':\n\n    prune\n\nlibrary(rpart.plot)\n\nWarning: package 'rpart.plot' was built under R version 4.4.2\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.4.2\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nurl &lt;- \"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\"\nbike_data &lt;- read.csv(url, fileEncoding = \"latin1\")\n\n# Check the data\nhead(bike_data)\n\n        Date Rented.Bike.Count Hour Temperature..C. Humidity...\n1 01/12/2017               254    0            -5.2          37\n2 01/12/2017               204    1            -5.5          38\n3 01/12/2017               173    2            -6.0          39\n4 01/12/2017               107    3            -6.2          40\n5 01/12/2017                78    4            -6.0          36\n6 01/12/2017               100    5            -6.4          37\n  Wind.speed..m.s. Visibility..10m. Dew.point.temperature..C.\n1              2.2             2000                     -17.6\n2              0.8             2000                     -17.6\n3              1.0             2000                     -17.7\n4              0.9             2000                     -17.6\n5              2.3             2000                     -18.6\n6              1.5             2000                     -18.7\n  Solar.Radiation..MJ.m2. Rainfall.mm. Snowfall..cm. Seasons    Holiday\n1                       0            0             0  Winter No Holiday\n2                       0            0             0  Winter No Holiday\n3                       0            0             0  Winter No Holiday\n4                       0            0             0  Winter No Holiday\n5                       0            0             0  Winter No Holiday\n6                       0            0             0  Winter No Holiday\n  Functioning.Day\n1             Yes\n2             Yes\n3             Yes\n4             Yes\n5             Yes\n6             Yes\n\n# Clean and rename the data\nbike_data_clean &lt;- bike_data %&gt;%\n    rename(\n    date = Date,\n    bike_count = `Rented.Bike.Count`,\n    hour = Hour,\n    temperature = `Temperature..C.`,\n    humidity = `Humidity...`,\n    wind_speed = `Wind.speed..m.s.`,\n    visibility = `Visibility..10m.`,\n    dew_point = `Dew.point.temperature..C.`,\n    solar_radiation = `Solar.Radiation..MJ.m2.`,\n    rainfall = `Rainfall.mm.`,\n    snowfall = `Snowfall..cm.`,\n    seasons = Seasons,\n    holiday = Holiday,\n    functioning_day = `Functioning.Day`\n  )\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Create initial split (75/25) stratified by seasons\nbike_split &lt;- initial_split(bike_data_clean, prop = 0.75, strata = seasons)\n\n# Create training and testing sets\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\n\n# Create 10-fold CV split on training data\nbike_folds &lt;- vfold_cv(bike_train, v = 10)\n\n# Check the dimensions of our splits\ncat(\"Dimensions of datasets:\\n\")\n\nDimensions of datasets:\n\ncat(\"Full data:\", nrow(bike_data_clean), \"rows\\n\")\n\nFull data: 8760 rows\n\ncat(\"Training set:\", nrow(bike_train), \"rows\\n\")\n\nTraining set: 6570 rows\n\ncat(\"Testing set:\", nrow(bike_test), \"rows\\n\")\n\nTesting set: 2190 rows\n\n# Check the structure of the CV folds\nbike_folds\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits             id    \n   &lt;list&gt;             &lt;chr&gt; \n 1 &lt;split [5913/657]&gt; Fold01\n 2 &lt;split [5913/657]&gt; Fold02\n 3 &lt;split [5913/657]&gt; Fold03\n 4 &lt;split [5913/657]&gt; Fold04\n 5 &lt;split [5913/657]&gt; Fold05\n 6 &lt;split [5913/657]&gt; Fold06\n 7 &lt;split [5913/657]&gt; Fold07\n 8 &lt;split [5913/657]&gt; Fold08\n 9 &lt;split [5913/657]&gt; Fold09\n10 &lt;split [5913/657]&gt; Fold10"
  },
  {
    "objectID": "HomeWrok9.html#fitting-mlr-models",
    "href": "HomeWrok9.html#fitting-mlr-models",
    "title": "Home Work 9",
    "section": "Fitting MLR Models",
    "text": "Fitting MLR Models\nFirst, let’s create some recipes. For the 1st recipe: • Let’s ignore the date variable for modeling (so we’ll need to remove that or give it a different ID) but use it to create a weekday/weekend (factor) variable. (See step 2 of the shinymodels tutorial! You can use step_date() then step_mutate() with a factor(if_else(…)) to create the variable. I then had to remove the intermediate variable created.) • Let’s standardize the numeric variables since their scales are pretty different. • Let’s create dummy variables for the seasons, holiday, and our new day type variable\n\n# Create first recipe\nrecipe1 &lt;- recipe(bike_count ~ ., data = bike_train) %&gt;%\n  # Convert date to Date format\n  step_mutate(date = lubridate::dmy(date)) %&gt;%\n # Create weekday/weekend variable from date\n step_date(date, features = \"dow\") %&gt;%\n step_mutate(\n   day_type = factor(if_else(\n     date_dow %in% c(\"Sat\", \"Sun\"), \n     \"weekend\", \n     \"weekday\"\n   ))\n ) %&gt;%\n # Remove the intermediate dow variable and date\n step_rm(date_dow, date) %&gt;%\n # Standardize numeric variables\n step_normalize(all_numeric_predictors()) %&gt;%\n # Create dummy variables\n step_dummy(all_nominal_predictors())\n\n# Print the recipe to check steps\nprint(recipe1)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 13\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: lubridate::dmy(date)\n\n\n• Date features from: date\n\n\n• Variable mutation for: factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"),\n  \"weekend\", \"weekday\"))\n\n\n• Variables removed: date_dow and date\n\n\n• Centering and scaling for: all_numeric_predictors()\n\n\n• Dummy variables from: all_nominal_predictors()\n\n# Check if recipe works by prepping it\nprep(recipe1) %&gt;%\n bake(new_data = NULL) %&gt;%\n glimpse()\n\nRows: 6,570\nColumns: 16\n$ hour                &lt;dbl&gt; -1.65328818, -1.50926857, -1.22122934, -0.93319011…\n$ temperature         &lt;dbl&gt; 0.8629573, 0.8211018, 0.7290195, 0.6871640, 0.6871…\n$ humidity            &lt;dbl&gt; 1.22149105, 1.22149105, 1.41862915, 1.41862915, 1.…\n$ wind_speed          &lt;dbl&gt; -1.178835879, -1.569270387, -1.374053133, -1.08122…\n$ visibility          &lt;dbl&gt; 0.7380877, 0.8250737, 0.2325845, -0.5273115, 0.493…\n$ dew_point           &lt;dbl&gt; 1.2254462, 1.1871984, 1.1642497, 1.1260019, 1.0418…\n$ solar_radiation     &lt;dbl&gt; -0.65622939, -0.65622939, -0.65622939, -0.65622939…\n$ rainfall            &lt;dbl&gt; -0.1291446, -0.1291446, -0.1291446, -0.1291446, -0…\n$ snowfall            &lt;dbl&gt; -0.1687671, -0.1687671, -0.1687671, -0.1687671, -0…\n$ bike_count          &lt;int&gt; 1075, 975, 514, 260, 511, 812, 925, 861, 981, 1086…\n$ seasons_Spring      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ seasons_Summer      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ seasons_Winter      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ holiday_No.Holiday  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ functioning_day_Yes &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ day_type_weekend    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\n\nFor the 2nd recipe: • Do the same steps as above. • Add in interactions between seasons and holiday, seasons and temp, temp and rainfall. For the seasons interactions, you can use starts_with() to create the proper interactions.\n\n# Create second recipe\nrecipe2 &lt;- recipe(bike_count ~ ., data = bike_train) %&gt;%\n  # Convert date to Date format\n  step_mutate(date = lubridate::dmy(date)) %&gt;%\n  step_date(date, features = \"dow\") %&gt;%\n  step_mutate(\n    day_type = factor(if_else(\n      date_dow %in% c(\"Sat\", \"Sun\"), \n      \"weekend\", \n      \"weekday\"\n    ))\n  ) %&gt;%\n  step_rm(date_dow, date) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;%  # Changed to one_hot encoding\n  # Simplified interactions\n  step_interact(terms = ~ temperature:rainfall)%&gt;%\n  # Handle rank deficiency\n  step_zv(all_predictors())\n\n# Print the recipe to check steps\nprint(recipe2)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 13\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: lubridate::dmy(date)\n\n\n• Date features from: date\n\n\n• Variable mutation for: factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"),\n  \"weekend\", \"weekday\"))\n\n\n• Variables removed: date_dow and date\n\n\n• Centering and scaling for: all_numeric_predictors()\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Interactions with: temperature:rainfall\n\n\n• Zero variance filter on: all_predictors()\n\n# Check if recipe works and examine the results\nprep(recipe2) %&gt;%\n bake(new_data = NULL) %&gt;%\n glimpse()\n\nRows: 6,570\nColumns: 21\n$ hour                   &lt;dbl&gt; -1.65328818, -1.50926857, -1.22122934, -0.93319…\n$ temperature            &lt;dbl&gt; 0.8629573, 0.8211018, 0.7290195, 0.6871640, 0.6…\n$ humidity               &lt;dbl&gt; 1.22149105, 1.22149105, 1.41862915, 1.41862915,…\n$ wind_speed             &lt;dbl&gt; -1.178835879, -1.569270387, -1.374053133, -1.08…\n$ visibility             &lt;dbl&gt; 0.7380877, 0.8250737, 0.2325845, -0.5273115, 0.…\n$ dew_point              &lt;dbl&gt; 1.2254462, 1.1871984, 1.1642497, 1.1260019, 1.0…\n$ solar_radiation        &lt;dbl&gt; -0.65622939, -0.65622939, -0.65622939, -0.65622…\n$ rainfall               &lt;dbl&gt; -0.1291446, -0.1291446, -0.1291446, -0.1291446,…\n$ snowfall               &lt;dbl&gt; -0.1687671, -0.1687671, -0.1687671, -0.1687671,…\n$ bike_count             &lt;int&gt; 1075, 975, 514, 260, 511, 812, 925, 861, 981, 1…\n$ seasons_Autumn         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ seasons_Spring         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ seasons_Summer         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ seasons_Winter         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ holiday_Holiday        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ holiday_No.Holiday     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ functioning_day_No     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ functioning_day_Yes    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ day_type_weekday       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ day_type_weekend       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ temperature_x_rainfall &lt;dbl&gt; -0.11144626, -0.10604084, -0.09414892, -0.08874…\n\n\n\n# Check names of interaction terms\nprocessed_data2 &lt;- prep(recipe2) %&gt;%\n bake(new_data = NULL)\n\ncat(\"\\nInteraction terms created:\\n\")\n\n\nInteraction terms created:\n\nnames(processed_data2)[grep(\"_x_\", names(processed_data2))]\n\n[1] \"temperature_x_rainfall\"\n\n\nThis recipe includes:\nAll transformations from recipe1 Interactions between:\nSeasons dummy variables and holiday Seasons dummy variables and mean temperature Mean temperature and total rainfall For the 3rd recipe: • Do the same as the 2nd recipe. • Add in quadratic terms for each numeric predictor\n\n# Create third recipe\nrecipe3 &lt;- recipe(bike_count ~ ., data = bike_train) %&gt;%\n  # Convert date to Date format\n  step_mutate(date = lubridate::dmy(date)) %&gt;%\n  \n  step_date(date, features = \"dow\") %&gt;%\n  step_mutate(\n    day_type = factor(if_else(\n      date_dow %in% c(\"Sat\", \"Sun\"), \n      \"weekend\", \n      \"weekday\"\n    ))\n  ) %&gt;%\n  step_rm(date_dow, date) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;%  # Changed to one_hot encoding\n  step_interact(terms = ~ temperature:rainfall) %&gt;%\n  # More selective with polynomial terms\n  step_poly(temperature, humidity, wind_speed, degree = 2)\n   \n# Print the recipe to check steps\nprint(recipe3)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 13\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: lubridate::dmy(date)\n\n\n• Date features from: date\n\n\n• Variable mutation for: factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"),\n  \"weekend\", \"weekday\"))\n\n\n• Variables removed: date_dow and date\n\n\n• Centering and scaling for: all_numeric_predictors()\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Interactions with: temperature:rainfall\n\n\n• Orthogonal polynomials on: temperature, humidity, wind_speed\n\n\n\n# Check if recipe works and examine the results\nprocessed_data3 &lt;- prep(recipe3) %&gt;%\n bake(new_data = NULL)\n\n# Look at the structure\nglimpse(processed_data3)\n\nRows: 6,570\nColumns: 24\n$ hour                   &lt;dbl&gt; -1.65328818, -1.50926857, -1.22122934, -0.93319…\n$ visibility             &lt;dbl&gt; 0.7380877, 0.8250737, 0.2325845, -0.5273115, 0.…\n$ dew_point              &lt;dbl&gt; 1.2254462, 1.1871984, 1.1642497, 1.1260019, 1.0…\n$ solar_radiation        &lt;dbl&gt; -0.65622939, -0.65622939, -0.65622939, -0.65622…\n$ rainfall               &lt;dbl&gt; -0.1291446, -0.1291446, -0.1291446, -0.1291446,…\n$ snowfall               &lt;dbl&gt; -0.1687671, -0.1687671, -0.1687671, -0.1687671,…\n$ bike_count             &lt;int&gt; 1075, 975, 514, 260, 511, 812, 925, 861, 981, 1…\n$ seasons_Autumn         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ seasons_Spring         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ seasons_Summer         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ seasons_Winter         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ holiday_Holiday        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ holiday_No.Holiday     &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ functioning_day_No     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ functioning_day_Yes    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ day_type_weekday       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ day_type_weekend       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ temperature_x_rainfall &lt;dbl&gt; -0.11144626, -0.10604084, -0.09414892, -0.08874…\n$ temperature_poly_1     &lt;dbl&gt; 0.010647305, 0.010130884, 0.008994759, 0.008478…\n$ temperature_poly_2     &lt;dbl&gt; -0.0009157760, -0.0018384016, -0.0037242432, -0…\n$ humidity_poly_1        &lt;dbl&gt; 0.0150709511, 0.0150709511, 0.0175032723, 0.017…\n$ humidity_poly_2        &lt;dbl&gt; 0.004605676, 0.004605676, 0.010331858, 0.010331…\n$ wind_speed_poly_1      &lt;dbl&gt; -0.0145446647, -0.0193619078, -0.0169532863, -0…\n$ wind_speed_poly_2      &lt;dbl&gt; 0.0127782235, 0.0253208758, 0.0187139988, 0.010…\n\n\n\n# Check names of quadratic terms\ncat(\"\\nQuadratic terms created:\\n\")\n\n\nQuadratic terms created:\n\nnames(processed_data3)[grep(\"_2$\", names(processed_data3))]\n\n[1] \"temperature_poly_2\" \"humidity_poly_2\"    \"wind_speed_poly_2\" \n\n\nAll transformations from recipe2 Quadratic terms (squared terms) for all numeric predictors:\nrainfall temperature humidity wind_speed visibility dew_point solar_rad total_snowfall\nEach numeric variable will now have both its linear and quadratic term, allowing for curved relationships with the response variable.\nNow set up our linear model fit to use the “lm” engine. Fit the models using 10 fold CV via fit_resamples() and consider the training set CV error to choose a best model.\n\n# Set up linear model with lm engine\nlm_model &lt;- linear_reg() %&gt;%\n set_engine(\"lm\")\n\n# Create workflows for each recipe\nworkflow1 &lt;- workflow() %&gt;%\n add_recipe(recipe1) %&gt;%\n add_model(lm_model)\n\nworkflow2 &lt;- workflow() %&gt;%\n add_recipe(recipe2) %&gt;%\n add_model(lm_model)\n\nworkflow3 &lt;- workflow() %&gt;%\n add_recipe(recipe3) %&gt;%\n add_model(lm_model)\n\n# Fit models using 10-fold CV\nset.seed(123)  # for reproducibility\n\n\ncv_results1 &lt;- workflow1 %&gt;%\n  fit_resamples(\n    resamples = bike_folds,\n    metrics = metric_set(rmse, rsq, mae),\n    control = control_resamples(save_pred = TRUE)\n  )\n\n# Fit model 2\ncv_results2 &lt;- workflow2 %&gt;%\n  fit_resamples(\n    resamples = bike_folds,\n    metrics = metric_set(rmse, rsq, mae),\n    control = control_resamples(save_pred = TRUE,\n                                extract = function(x) predict(x, rankdeficient = \"NA\"))\n  )\n\n→ A | error:   argument \"new_data\" is missing, with no default\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x10   B: x10\n\n# Fit model 3\ncv_results3 &lt;- workflow3 %&gt;%\n  fit_resamples(\n    resamples = bike_folds,\n    metrics = metric_set(rmse, rsq, mae),\n    control = control_resamples(save_pred = TRUE,\n                                extract = function(x) predict(x, rankdeficient = \"NA\"))\n  )\n\n→ A | error:   argument \"new_data\" is missing, with no default\nThere were issues with some computations   A: x1\n→ B | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x10   B: x10\n\n# Collect and compare CV results\ncat(\"Model 1 CV Results:\\n\")\n\nModel 1 CV Results:\n\ncollect_metrics(cv_results1)\n\n# A tibble: 3 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   324.       10 3.84    Preprocessor1_Model1\n2 rmse    standard   432.       10 5.66    Preprocessor1_Model1\n3 rsq     standard     0.553    10 0.00791 Preprocessor1_Model1\n\ncat(\"\\nModel 2 CV Results:\\n\")\n\n\nModel 2 CV Results:\n\ncollect_metrics(cv_results2)\n\n# A tibble: 3 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   324.       10 3.82    Preprocessor1_Model1\n2 rmse    standard   432.       10 5.66    Preprocessor1_Model1\n3 rsq     standard     0.553    10 0.00789 Preprocessor1_Model1\n\ncat(\"\\nModel 3 CV Results:\\n\")\n\n\nModel 3 CV Results:\n\ncollect_metrics(cv_results3)\n\n# A tibble: 3 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   315.       10 3.68    Preprocessor1_Model1\n2 rmse    standard   421.       10 5.19    Preprocessor1_Model1\n3 rsq     standard     0.575    10 0.00667 Preprocessor1_Model1\n\n# Compare models side by side\n\ncombined_results &lt;- bind_rows(\n  collect_metrics(cv_results1) %&gt;% mutate(model = \"Model 1\"),\n  collect_metrics(cv_results2) %&gt;% mutate(model = \"Model 2\"),\n  collect_metrics(cv_results3) %&gt;% mutate(model = \"Model 3\")\n)\n\nTo determine the best model among the three sets of results, we need to compare their performance metrics. Let’s focus on the RMSE (Root Mean Squared Error) and R-squared values, as they provide a good indication of the model’s predictive accuracy and goodness of fit.\nModel 1:\nRMSE: 432.4479006 R-squared: 0.5515968\nModel 2:\nRMSE: 432.6638342 R-squared: 0.5511493\nModel 3:\nRMSE: 421.644854 R-squared: 0.573702\nComparing the three models:\nModel 3 has the lowest RMSE value of 421.644854, which indicates that it has the smallest average prediction error among the three models. A lower RMSE suggests better predictive accuracy. Model 3 also has the highest R-squared value of 0.573702, meaning that it explains the highest proportion of variance in the target variable among the three models. A higher R-squared indicates a better fit of the model to the data.\nBased on these results, Model 3 appears to be the best-performing model among the three. It has the lowest RMSE and the highest R-squared, suggesting that it has better predictive accuracy and explains more variability in the target variable compared to the other two models. However, it’s important to note that the differences in performance metrics between the models are relatively small. The RMSE values are close to each other, and the R-squared values are also similar.\nFor the homework 9 I will used model 3\n\n# Define the model recipes and workflows\n\n# LASSO model with pre-processing steps\n\nlasso_recipe &lt;- recipe(bike_count ~ ., data = bike_train) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())  # Handle categorical variables\n\nlasso_model &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\nlasso_workflow &lt;- workflow() %&gt;%\n  add_recipe(lasso_recipe) %&gt;%\n  add_model(lasso_model)\n\n# Regression Tree model with preprocessing steps\ntree_recipe &lt;- recipe(bike_count ~ ., data = bike_train) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())\n\ntree_model &lt;- decision_tree(cost_complexity = tune(), min_n = tune()) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\ntree_workflow &lt;- workflow() %&gt;%\n  add_recipe(tree_recipe) %&gt;%\n  add_model(tree_model)\n\n\n# Bagged Tree model with preprocessing steps\nbagged_recipe &lt;- recipe(bike_count ~ ., data = bike_train) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())\n\n# Correct the model specification (without `trees` argument)\nbagged_model &lt;- bag_tree(tree_depth = 5, cost_complexity = tune(), min_n = tune()) %&gt;%   \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"regression\")\n\nbagged_workflow &lt;- workflow() %&gt;%\n  add_recipe(bagged_recipe) %&gt;%\n  add_model(bagged_model)\n\n# Random Forest model with preprocessing steps\nrf_recipe &lt;- recipe(bike_count ~ ., data = bike_train) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors())\n\nrf_model &lt;- rand_forest(mtry = tune(), min_n = tune(), trees = 5) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(rf_recipe) %&gt;%\n  add_model(rf_model)\n\n# Define the minimal tuning grids for all models\nlasso_grid_minimal &lt;- expand.grid(penalty = 10^seq(-3, -1, length.out = 3))\ntree_grid_minimal &lt;- expand.grid(cost_complexity = 10^seq(-2, -1, length.out = 2),\n                                 min_n = c(5, 10))\nrf_grid_minimal &lt;- expand.grid(mtry = c(2, 4), min_n = c(5, 10))\n\n# Define an extremely small tuning grid for the Bagged Tree model\nbagged_grid_fast &lt;- expand.grid(cost_complexity = 10^-2,\n                                min_n = c(5))\n\n# Create a smaller set of resamples for faster tuning\nset.seed(123)\nbike_folds_fast &lt;- vfold_cv(bike_train, v = 3)\n\n# Tune the models\nlasso_tuned &lt;- tune_grid(\n  lasso_workflow,\n  resamples = bike_folds_fast,\n  grid = lasso_grid_minimal,\n  metrics = metric_set(rmse, rsq, mae)\n)\n\ntree_tuned &lt;- tune_grid(\n  tree_workflow,\n  resamples = bike_folds_fast,\n  grid = tree_grid_minimal,\n  metrics = metric_set(rmse, rsq, mae)\n)\n\nbagged_tuned &lt;- tune_grid(\n  bagged_workflow,\n  resamples = bike_folds_fast,\n  grid = bagged_grid_fast,\n  metrics = metric_set(rmse, rsq, mae)\n)\n\nrf_tuned &lt;- tune_grid(\n  rf_workflow,\n  resamples = bike_folds_fast,\n  grid = rf_grid_minimal,\n  metrics = metric_set(rmse, rsq, mae)\n)\n\n# Select the best model for each tuned model\nbest_lasso &lt;- lasso_tuned %&gt;% select_best(metric = \"rmse\")\nbest_tree &lt;- tree_tuned %&gt;% select_best(metric = \"rmse\")\nbest_bagged &lt;- bagged_tuned %&gt;% select_best(metric = \"rmse\")\nbest_rf &lt;- rf_tuned %&gt;% select_best(metric = \"rmse\")\n\n# Finalize the workflows with the best models\nfinal_lasso_workflow &lt;- lasso_workflow %&gt;% finalize_workflow(best_lasso)\nfinal_tree_workflow &lt;- tree_workflow %&gt;% finalize_workflow(best_tree)\nfinal_bagged_workflow &lt;- bagged_workflow %&gt;% finalize_workflow(best_bagged)\nfinal_rf_workflow &lt;- rf_workflow %&gt;% finalize_workflow(best_rf)\n\n# Fit the finalized models on the full training data\nfinal_lasso_fit &lt;- final_lasso_workflow %&gt;% fit(data = bike_train)\nfinal_tree_fit &lt;- final_tree_workflow %&gt;% fit(data = bike_train)\nfinal_bagged_fit &lt;- final_bagged_workflow %&gt;% fit(data = bike_train)\nfinal_rf_fit &lt;- final_rf_workflow %&gt;% fit(data = bike_train)\nfinal_mlr_fit &lt;- workflow3 %&gt;% fit(data=bike_train)\n\n\n\nlibrary(yardstick)\nlibrary(purrr)\n\n# Compare the models on the test set\n# Compare the models on the test set\ntest_results &lt;- bike_test %&gt;%\n  bind_cols(\n    lasso_pred = predict(final_lasso_fit, new_data = bike_test)$.pred,\n    tree_pred = predict(final_tree_fit, new_data = bike_test)$.pred,\n    bagged_pred = predict(final_bagged_fit, new_data = bike_test)$.pred,\n    rf_pred = predict(final_rf_fit, new_data = bike_test)$.pred,\n   \n  )\n\ntest_metrics &lt;- imap_dfr(\n  list(\n    LASSO = test_results$lasso_pred,\n    `Regression Tree` = test_results$tree_pred,\n    `Bagged Tree` = test_results$bagged_pred,\n    `Random Forest` = test_results$rf_pred\n    \n  ),\n  ~ tibble(\n    model = .y,\n    rmse = rmse_vec(test_results$bike_count, .x),\n    mae = mae_vec(test_results$bike_count, .x)\n  )\n)\n\nprint(test_metrics)\n\n# A tibble: 4 × 3\n  model            rmse   mae\n  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1 LASSO            382.  292.\n2 Regression Tree  373.  265.\n3 Bagged Tree      356.  251.\n4 Random Forest    386.  288.\n\n# Select the best overall model\nbest_model &lt;- test_metrics %&gt;%\n  arrange(rmse) %&gt;%\n  slice(1) %&gt;%\n  pull(model)\n\ncat(\"The best overall model based on RMSE is:\", best_model, \"\\n\")\n\nThe best overall model based on RMSE is: Bagged Tree \n\n# Fit the best model on the entire dataset\nbest_fit &lt;- switch(best_model,\n                   \"LASSO\" = final_lasso_workflow,\n                   \"Regression Tree\" = final_tree_workflow,\n                   \"Bagged Tree\" = final_bagged_workflow,\n                   \"Random Forest\" = final_rf_workflow,\n                   \n) %&gt;%\n  fit(data = bike_data_clean)"
  }
]